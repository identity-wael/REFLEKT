import { Tabs, Callout } from 'nextra/components'

# Troubleshooting & FAQ

This guide helps you diagnose and resolve common issues with REFLEKT Console, including AI SDK v5 compatibility, message persistence, and TypeScript strict mode issues.

## Quick Diagnostics

### System Health Check

```bash
# Check GPT-4o API connectivity
curl https://api.openai.com/v1/models/gpt-4o \
  -H "Authorization: Bearer \$OPENAI_API_KEY"

# Check Supabase database connection
curl https://YOUR_PROJECT.supabase.co/rest/v1/ \
  -H "apikey: \$NEXT_PUBLIC_SUPABASE_ANON_KEY" \
  -H "Authorization: Bearer \$NEXT_PUBLIC_SUPABASE_ANON_KEY"

# Check Vercel deployment status
curl https://api.vercel.com/v6/deployments/YOUR_DEPLOYMENT
```

### Performance Monitoring

```typescript
import { openai } from '@ai-sdk/openai';
import { createClient } from '@supabase/supabase-js';

export async function healthCheck() {
  const supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  );

  // Check GPT-4o API
  const startTime = Date.now();
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: 'test' }],
      max_tokens: 10
    });
    const latency = Date.now() - startTime;
    console.log(`GPT-4o Response Time: ${latency}ms`);
  } catch (error) {
    console.error('GPT-4o API Error:', error);
  }

  // Check Supabase connection
  try {
    const { data, error } = await supabase.from('users').select('count').single();
    if (error) throw error;
    console.log('Supabase connection: OK');
  } catch (error) {
    console.error('Supabase Error:', error);
  }

  // Check alternative models
  const models = ['claude-3-opus', 'gemini-1.5-pro'];
  for (const model of models) {
    const available = await checkModelAvailability(model);
    console.log(`${model}: ${available ? 'Available' : 'Unavailable'}`);
  }
}
```

## Common Issues

### AI SDK v5 Issues

<Tabs items={['Message Persistence', 'Type Errors', 'Streaming Issues']}>
  <Tabs.Tab>
    **Message Persistence After Reload**

    **Problem**: Chat messages don't persist after page reload

    **Symptoms**:
    - Messages appear in sidebar history
    - Database contains messages
    - Main chat window is empty after refresh

    **Solution**: Synchronize MessagesProvider with useChat hook

    ```typescript
    // app/components/chat/use-chat-core.ts
    export function useChatCore({ initialMessages, ... }) {
      const hasSyncedInitialMessagesRef = useRef(false)

      // Critical: Sync messages from database
      useEffect(() => {
        if (initialMessages.length > 0 &&
            !hasSyncedInitialMessagesRef.current &&
            status === "ready") {

          // Convert Message[] to UIMessage[]
          const uiMessages = initialMessages.map(msg => ({
            id: msg.id,
            role: msg.role,
            parts: [{ type: 'text', text: msg.content }]
          }))

          setMessages(uiMessages)
          hasSyncedInitialMessagesRef.current = true
        }
      }, [initialMessages, messages.length, setMessages, status])
    }
    ```

    **Additional Checks**:
    ```typescript
    // Verify messages are loaded from database
    console.log('Messages from DB:', initialMessages.length)
    console.log('Messages in UI:', messages.length)
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **TypeScript Type Errors**

    **Problem**: Type incompatibilities with AI SDK v5

    **Common Errors**:
    - `Property 'content' does not exist on type 'UIMessage'`
    - `Type 'LanguageModelV1' is not assignable to type 'LanguageModel'`
    - `Property 'isLoading' does not exist on type 'UseChatHelpers'`

    **Solutions**:

    1. **Message Type Conversion**
    ```typescript
    // Convert between Message and UIMessage
    function convertToUIMessage(message: Message): UIMessage {
      return {
        id: message.id,
        role: message.role as 'user' | 'assistant' | 'system',
        parts: [{ type: 'text', text: message.content }]
      }
    }

    // Extract text from UIMessage
    function getTextContent(message: UIMessage): string {
      return message.parts
        .filter((part): part is { type: 'text'; text: string } =>
          part.type === 'text'
        )
        .map(part => part.text)
        .join('')
    }
    ```

    2. **Provider Types**
    ```typescript
    // Use LanguageModel instead of LanguageModelV1
    import type { LanguageModel } from 'ai'
    // NOT: import type { LanguageModelV1 } from 'ai'
    ```

    3. **Type Assertions for Compatibility**
    ```typescript
    // When strict types are incompatible
    const chatHelpers = useChat({
      initialMessages: [],
      onFinish: cacheAndAddMessage as any,
    } as any)

    const isLoading = (chatHelpers as any).isLoading
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Streaming Response Issues**

    **Problem**: sendMessage format error
    ```
    TypeError: Cannot use 'in' operator to search for 'text' in hello
    ```

    **Solution**: Use object format instead of string
    ```typescript
    // Wrong - AI SDK v3/v4 format
    await sendMessage('Hello')

    // Correct - AI SDK v5 format
    await sendMessage({ text: 'Hello' }, options)
    ```

    **Problem**: Streaming status not updating

    **Solution**: Access status from extended properties
    ```typescript
    const chatHelpers = useChat({...})
    const status = (chatHelpers as any).status || 'ready'

    // Use status for UI updates
    {status === 'streaming' && <Loader />}
    {status === 'ready' && <Message />}
    ```
  </Tabs.Tab>
</Tabs>

<Callout type="warning">
**Important**: Always run `pnpm type-check` after making changes to ensure TypeScript compatibility.
</Callout>

### GPT-4o API Problems

<Tabs items={['API Errors', 'Rate Limiting', 'Streaming Issues', 'Vision Problems']}>
  <Tabs.Tab>
    **GPT-4o API Errors**

    **Problem**: Authentication failed
    ```json
    {
      "error": {
        "message": "Incorrect API key provided",
        "type": "invalid_request_error",
        "code": "invalid_api_key"
      }
    }
    ```

    **Solutions**:
    1. **Verify API Key**
       ```typescript
       // Check environment variable
       console.log('API Key exists:', !!process.env.OPENAI_API_KEY);
       console.log('Key prefix:', process.env.OPENAI_API_KEY?.substring(0, 7));

       // Ensure key is properly formatted
       const apiKey = process.env.OPENAI_API_KEY?.trim();
       ```

    2. **Update Environment Variables**
       ```bash
       # Local development
       echo "OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx" >> .env.local

       # Vercel production
       vercel env add OPENAI_API_KEY production
       ```

    3. **Check API Key Permissions**
       ```typescript
       // Test API key with minimal request
       import OpenAI from 'openai';

       const openai = new OpenAI({
         apiKey: process.env.OPENAI_API_KEY
       });

       try {
         const test = await openai.chat.completions.create({
           model: 'gpt-4o',
           messages: [{ role: 'user', content: 'Hi' }],
           max_tokens: 10
         });
         console.log('API key valid');
       } catch (error) {
         console.error('API key invalid:', error);
       }
       ```

    **Problem**: Model not available
    ```json
    {
      "error": {
        "message": "The model `gpt-4o` does not exist",
        "type": "invalid_request_error"
      }
    }
    ```

    **Solutions**:
    1. **Check Model Access**
       ```typescript
       // List available models
       const models = await openai.models.list();
       const hasGPT4o = models.data.some(m => m.id === 'gpt-4o');

       if (!hasGPT4o) {
         console.log('GPT-4o not available, using fallback');
         // Fall back to gpt-4-turbo or gpt-3.5-turbo
       }
       ```

    2. **Use Vercel AI SDK with Fallback**
       ```typescript
       import { openai } from '@ai-sdk/openai';
       import { anthropic } from '@ai-sdk/anthropic';

       async function getAIResponse(messages: any[]) {
         try {
           // Try GPT-4o first
           return await streamText({
             model: openai('gpt-4o'),
             messages
           });
         } catch (error) {
           // Fallback to Claude
           return await streamText({
             model: anthropic('claude-3-opus-20240229'),
             messages
           });
         }
       }
       ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Rate Limiting Issues**

    **Problem**: Too many requests error
    ```json
    {
      "error": {
        "message": "Rate limit reached",
        "type": "rate_limit_error"
      }
    }
    ```

    **Solutions**:
    1. **Implement Exponential Backoff**
       ```typescript
       async function withRetry<T>(
         fn: () => Promise<T>,
         maxRetries = 3
       ): Promise<T> {
         let lastError;

         for (let i = 0; i < maxRetries; i++) {
           try {
             return await fn();
           } catch (error: any) {
             lastError = error;

             if (error.status === 429) {
               // Rate limited - wait exponentially
               const delay = Math.min(1000 * Math.pow(2, i), 30000);
               await new Promise(resolve => setTimeout(resolve, delay));
             } else {
               throw error;
             }
           }
         }
         throw lastError;
       }
       ```

    2. **Use Request Queue**
       ```typescript
       import pLimit from 'p-limit';

       const gpt4oLimit = pLimit(5); // Max 5 concurrent requests

       export async function queueGPT4oRequest(request: () => Promise<any>) {
         return gpt4oLimit(request);
       }
       ```

    **Problem**: Token limit exceeded
    ```json
    {
      "error": {
        "message": "This model's maximum context length is 128000 tokens",
        "type": "invalid_request_error"
      }
    }
    ```

    **Solutions**:
    1. **Implement Token Counting**
       ```typescript
       import { encoding_for_model } from 'tiktoken';

       function countTokens(text: string): number {
         const encoding = encoding_for_model('gpt-4');
         const tokens = encoding.encode(text);
         encoding.free();
         return tokens.length;
       }

       // Trim messages if needed
       function trimToTokenLimit(messages: any[], limit = 120000) {
         let totalTokens = 0;
         const trimmed = [];

         for (const msg of messages.reverse()) {
           const tokens = countTokens(JSON.stringify(msg));
           if (totalTokens + tokens > limit) break;
           totalTokens += tokens;
           trimmed.unshift(msg);
         }

         return trimmed;
       }
       ```

    2. **Use Conversation Summarization**
       ```typescript
       async function summarizeOldMessages(messages: any[]) {
         if (messages.length > 20) {
           const oldMessages = messages.slice(0, -10);
           const summary = await openai.chat.completions.create({
             model: 'gpt-3.5-turbo', // Use cheaper model for summarization
             messages: [
               {
                 role: 'user',
                 content: `Summarize: ${JSON.stringify(oldMessages)}`
               }
             ],
             max_tokens: 500
           });

           return [
             { role: 'assistant', content: summary.choices[0].message.content },
             ...messages.slice(-10)
           ];
         }
         return messages;
       }
       ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Streaming Response Issues**

    **Problem**: SSE connection drops
    ```
    Error: EventSource connection lost
    ```

    **Solutions**:
    1. **Implement Reconnection Logic**
       ```typescript
       class RobustEventSource {
         private eventSource: EventSource | null = null;
         private url: string;
         private reconnectAttempts = 0;

         constructor(url: string) {
           this.url = url;
           this.connect();
         }

         connect() {
           this.eventSource = new EventSource(this.url);

           this.eventSource.onopen = () => {
             this.reconnectAttempts = 0;
             console.log('SSE connected');
           };

           this.eventSource.onerror = () => {
             this.eventSource?.close();

             // Exponential backoff reconnection
             const delay = Math.min(1000 * Math.pow(2, this.reconnectAttempts), 30000);
             setTimeout(() => this.connect(), delay);
             this.reconnectAttempts++;
           };
         }
       }
       ```

    2. **Handle Stream Interruptions**
       ```typescript
       import { streamText } from 'ai';

       const response = await streamText({
         model: openai('gpt-4o'),
         messages,
         onFinish: ({ text, usage }) => {
           // Save completed response
           console.log('Stream completed:', { text, usage });
         }
       });

       try {
         for await (const chunk of response.textStream) {
           // Process chunk
           process.stdout.write(chunk);
         }
       } catch (error) {
         console.error('Stream interrupted:', error);
         // Resume from last position if needed
       }
       ```

    **Problem**: WebSocket disconnections (Realtime API)
    ```
    Error: WebSocket connection closed unexpectedly
    ```

    **Solutions**:
    1. **Implement Heartbeat**
       ```typescript
       class RealtimeManager {
         private ws: WebSocket | null = null;
         private heartbeatInterval: NodeJS.Timeout | null = null;

         connect(url: string) {
           this.ws = new WebSocket(url);

           this.ws.onopen = () => {
             // Start heartbeat
             this.heartbeatInterval = setInterval(() => {
               if (this.ws?.readyState === WebSocket.OPEN) {
                 this.ws.send(JSON.stringify({ type: 'ping' }));
               }
             }, 30000); // Every 30 seconds
           };

           this.ws.onclose = () => {
             if (this.heartbeatInterval) {
               clearInterval(this.heartbeatInterval);
             }
             // Reconnect after delay
             setTimeout(() => this.connect(url), 5000);
           };
         }
       }
       ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Vision API Problems**

    **Problem**: Image analysis failed
    ```json
    {
      "error": {
        "message": "Invalid image format or size",
        "type": "invalid_request_error"
      }
    }
    ```

    **Solutions**:
    1. **Validate and Resize Images**
       ```typescript
       import sharp from 'sharp';

       async function prepareImageForGPT4o(
         imageBuffer: Buffer,
         maxSize = 20 * 1024 * 1024 // 20MB
       ): Promise<string> {
         // Check size
         if (imageBuffer.length > maxSize) {
           // Resize if too large
           imageBuffer = await sharp(imageBuffer)
             .resize(2048, 2048, {
               fit: 'inside',
               withoutEnlargement: true
             })
             .jpeg({ quality: 85 })
             .toBuffer();
         }

         // Convert to base64
         return `data:image/jpeg;base64,${imageBuffer.toString('base64')}`;
       }
       ```

    2. **Handle Vision Requests**
       ```typescript
       async function analyzeImage(imageUrl: string, prompt: string) {
         try {
           const response = await openai.chat.completions.create({
             model: 'gpt-4o',
             messages: [
               {
                 role: 'user',
                 content: [
                   { type: 'text', text: prompt },
                   { type: 'image_url', image_url: { url: imageUrl } }
                 ]
               }
             ],
             max_tokens: 1000
           });

           return response.choices[0].message.content;
         } catch (error) {
           console.error('Vision API error:', error);
           // Fallback to alternative vision model
           return await analyzeWithGeminiVision(imageUrl, prompt);
         }
       }
       ```
  </Tabs.Tab>
</Tabs>

### Supabase Integration Issues

#### Database Connection Problems

**Problem**: Supabase connection timeout
```
Error: Connection to database timed out after 5000ms
```

**Solutions**:
1. **Configure Connection Pool**
   ```typescript
   import { createClient } from '@supabase/supabase-js';

   const supabase = createClient(
     process.env.NEXT_PUBLIC_SUPABASE_URL!,
     process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
     {
       auth: {
         persistSession: true,
         autoRefreshToken: true
       },
       db: {
         schema: 'public'
       },
       global: {
         headers: {
           'x-connection-pool': 'true'
         }
       }
     }
   );
   ```

2. **Implement Connection Retry**
   ```typescript
   async function withSupabaseRetry<T>(
     operation: () => Promise<T>,
     maxRetries = 3
   ): Promise<T> {
     for (let i = 0; i < maxRetries; i++) {
       try {
         return await operation();
       } catch (error: any) {
         if (i === maxRetries - 1) throw error;

         if (error.message?.includes('timeout')) {
           await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
         } else {
           throw error;
         }
       }
     }
     throw new Error('Max retries exceeded');
   }
   ```

#### Realtime Subscription Issues

**Problem**: Realtime updates not working
```
Error: Realtime subscription failed
```

**Solutions**:
1. **Setup Realtime Properly**
   ```typescript
   // Enable realtime for table in Supabase dashboard first

   const channel = supabase
     .channel('messages')
     .on(
       'postgres_changes',
       {
         event: 'INSERT',
         schema: 'public',
         table: 'messages'
       },
       (payload) => {
         console.log('New message:', payload.new);
       }
     )
     .subscribe((status) => {
       if (status === 'SUBSCRIBED') {
         console.log('Realtime connected');
       }
     });

   // Clean up subscription
   return () => {
     supabase.removeChannel(channel);
   };
   ```

2. **Handle Reconnection**
   ```typescript
   class RealtimeManager {
     private channels: Map<string, any> = new Map();

     subscribeWithReconnect(
       channelName: string,
       callback: (payload: any) => void
     ) {
       let retries = 0;

       const subscribe = () => {
         const channel = supabase
           .channel(channelName)
           .on('postgres_changes', {
             event: '*',
             schema: 'public'
           }, callback)
           .subscribe((status) => {
             if (status === 'SUBSCRIBED') {
               retries = 0;
             } else if (status === 'CHANNEL_ERROR') {
               // Retry with exponential backoff
               const delay = Math.min(1000 * Math.pow(2, retries), 30000);
               setTimeout(subscribe, delay);
               retries++;
             }
           });

         this.channels.set(channelName, channel);
       };

       subscribe();
     }
   }
   ```

#### Storage Upload Issues

**Problem**: File upload failed
```
Error: new row violates row-level security policy
```

**Solutions**:
1. **Configure Storage Policies**
   ```sql
   -- In Supabase SQL Editor
   CREATE POLICY "Users can upload files" ON storage.objects
     FOR INSERT WITH CHECK (
       bucket_id = 'chat-attachments' AND
       auth.uid()::text = (storage.foldername(name))[1]
     );

   CREATE POLICY "Users can view own files" ON storage.objects
     FOR SELECT USING (
       bucket_id = 'chat-attachments' AND
       auth.uid()::text = (storage.foldername(name))[1]
     );
   ```

2. **Handle File Uploads**
   ```typescript
   async function uploadFile(file: File, userId: string) {
     const fileName = `${userId}/${Date.now()}-${file.name}`;

     const { data, error } = await supabase.storage
       .from('chat-attachments')
       .upload(fileName, file, {
         cacheControl: '3600',
         upsert: false
       });

     if (error) {
       console.error('Upload error:', error);
       throw error;
     }

     // Get public URL
     const { data: { publicUrl } } = supabase.storage
       .from('chat-attachments')
       .getPublicUrl(fileName);

     return publicUrl;
   }
   ```

### Alternative Model Issues

#### Claude Integration Problems

**Problem**: Claude API not responding
```json
{
  "error": {
    "type": "api_error",
    "message": "Claude API unavailable"
  }
}
```

**Solution**: Implement fallback to GPT-4o
```typescript
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';

async function getAIResponseWithFallback(messages: any[]) {
  const models = [
    { provider: anthropic('claude-3-opus-20240229'), name: 'Claude' },
    { provider: openai('gpt-4o'), name: 'GPT-4o' },
    { provider: google('gemini-1.5-pro'), name: 'Gemini' }
  ];

  for (const { provider, name } of models) {
    try {
      console.log(`Trying ${name}...`);
      return await streamText({
        model: provider,
        messages
      });
    } catch (error) {
      console.error(`${name} failed:`, error);
      continue;
    }
  }

  throw new Error('All AI models unavailable');
}
```

### HeyGen Avatar Issues

**Problem**: Avatar session creation failed
```
Error: Failed to create HeyGen session
```

**Solutions**:
1. **Validate HeyGen Configuration**
   ```typescript
   async function createAvatarSession() {
     try {
       const response = await fetch('https://api.heygen.com/v1/streaming.new', {
         method: 'POST',
         headers: {
           'X-Api-Key': process.env.HEYGEN_API_KEY!,
           'Content-Type': 'application/json'
         },
         body: JSON.stringify({
           avatar_id: 'josh_lite3_20230714',
           voice: {
             voice_id: 'en-US-BrianNeural'
           }
         })
       });

       if (!response.ok) {
         throw new Error(`HeyGen error: ${response.statusText}`);
       }

       return await response.json();
     } catch (error) {
       console.error('Avatar session error:', error);
       // Disable avatar feature gracefully
       return null;
     }
   }
   ```

### Performance Optimization

#### Slow Response Times

**Problem**: Chat responses taking too long
```
Warning: Response time exceeds 3 seconds
```

**Solutions**:
1. **Implement Response Caching**
   ```typescript
   import { Redis } from '@upstash/redis';

   const redis = new Redis({
     url: process.env.UPSTASH_REDIS_REST_URL!,
     token: process.env.UPSTASH_REDIS_REST_TOKEN!
   });

   async function getCachedResponse(
     prompt: string,
     model: string
   ): Promise<any> {
     const cacheKey = `response:${model}:${prompt.slice(0, 100)}`;

     // Check cache
     const cached = await redis.get(cacheKey);
     if (cached) {
       console.log('Cache hit');
       return cached;
     }

     // Generate new response
     const response = await generateResponse(prompt, model);

     // Cache for 5 minutes
     await redis.setex(cacheKey, 300, response);

     return response;
   }
   ```

2. **Optimize Database Queries**
   ```typescript
   // Use select to limit fields
   const messages = await supabase
     .from('messages')
     .select('id, content, role, created_at')
     .eq('chat_id', chatId)
     .order('created_at', { ascending: false })
     .limit(50);

   // Use RPC for complex queries
   const { data } = await supabase
     .rpc('get_chat_with_messages', {
       chat_id: chatId,
       message_limit: 50
     });
   ```

## Frequently Asked Questions

### General Questions

**Q: What AI models does REFLEKT Console support?**

A: REFLEKT Console supports:
- **Primary**: GPT-4o (OpenAI) - Advanced reasoning, vision, and tools
- **Alternatives**: Claude 3 Opus, Gemini 1.5 Pro, Mistral Large
- **Local**: Ollama integration for privacy-focused deployments
- **Specialized**: GPT-4o Realtime for voice, HeyGen for avatars

**Q: What are the daily message limits?**

A: Message limits by subscription tier:
- **Free**: 10 messages/day
- **Pro**: 100 messages/day
- **Enterprise**: Unlimited messages
- Limits reset at midnight UTC

**Q: How do I reduce GPT-4o costs?**

A: Cost optimization strategies:

```typescript
// 1. Use streaming to show progress
const stream = await streamText({
  model: openai('gpt-4o'),
  messages,
  temperature: 0.7, // Lower temperature = more focused
  maxTokens: 1000   // Limit response length
});

// 2. Cache common responses
const cachedResponse = await checkCache(prompt);
if (cachedResponse) return cachedResponse;

// 3. Use cheaper models for simple tasks
const model = isComplexQuery(prompt) ? 'gpt-4o' : 'gpt-3.5-turbo';

// 4. Implement usage tracking
await trackUsage(userId, tokens, cost);
```

### Technical Questions

**Q: How do I handle network interruptions?**

A: Implement robust error handling:

```typescript
// Automatic retry with exponential backoff
async function reliableRequest(fn: () => Promise<any>) {
  const maxRetries = 3;
  let lastError;

  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      const delay = Math.min(1000 * Math.pow(2, i), 10000);
      await new Promise(r => setTimeout(r, delay));
    }
  }

  throw lastError;
}

// Usage
const response = await reliableRequest(() =>
  openai.chat.completions.create({
    model: 'gpt-4o',
    messages
  })
);
```

**Q: How do I optimize for mobile devices?**

A: Mobile optimization techniques:

```typescript
// 1. Reduce payload size
const mobileMessages = messages.slice(-10); // Limit history

// 2. Use progressive loading
const [initialData, setInitialData] = useState(null);
const [fullData, setFullData] = useState(null);

useEffect(() => {
  // Load essential data first
  loadInitialData().then(setInitialData);
  // Load rest in background
  loadFullData().then(setFullData);
}, []);

// 3. Implement offline support
if (!navigator.onLine) {
  return getCachedData();
}
```

**Q: How do I debug Supabase RLS policies?**

A: Debug Row Level Security issues:

```sql
-- Check current user
SELECT auth.uid();

-- Test RLS policies
SET LOCAL ROLE authenticated;
SET LOCAL auth.uid TO 'user-uuid-here';

-- Now test your queries
SELECT * FROM messages WHERE chat_id = 'test';

-- Check policy definitions
SELECT * FROM pg_policies WHERE tablename = 'messages';
```

### Deployment Questions

**Q: How do I deploy to Vercel?**

A: Deploy with proper configuration:

```bash
# Install Vercel CLI
pnpm add -g vercel

# Set environment variables
vercel env add OPENAI_API_KEY production
vercel env add NEXT_PUBLIC_SUPABASE_URL production
vercel env add NEXT_PUBLIC_SUPABASE_ANON_KEY production

# Deploy
vercel --prod
```

**Q: How do I monitor production errors?**

A: Set up error monitoring:

```typescript
// Install Sentry
pnpm add @sentry/nextjs

// Configure in next.config.js
const { withSentryConfig } = require('@sentry/nextjs');

module.exports = withSentryConfig(
  nextConfig,
  {
    silent: true,
    org: 'your-org',
    project: 'reflekt-console'
  }
);

// Capture errors
import * as Sentry from '@sentry/nextjs';

try {
  // Your code
} catch (error) {
  Sentry.captureException(error);
  throw error;
}
```

## Getting Help

### Support Channels

1. **Documentation**: [docs.reflekt.sh](https://docs.reflekt.sh)
2. **GitHub Issues**: [github.com/reflekt-ai/console/issues](https://github.com/reflekt-ai/console/issues)
3. **Discord Community**: [discord.gg/reflekt](https://discord.gg/reflekt)
4. **Email Support**: [support@reflekt.sh](mailto:support@reflekt.sh)

### Before Contacting Support

Please include:

1. **System Information**
   ```bash
   # Get system info
   npx envinfo --system --npmPackages '{@ai-sdk/*,@supabase/*,next}' --binaries
   ```

2. **Error Logs**
   ```typescript
   // Enable debug logging
   localStorage.setItem('debug', 'ai:*,supabase:*');

   // Capture console output
   console.log('Error context:', {
     timestamp: new Date().toISOString(),
     error: error.message,
     stack: error.stack,
     model: currentModel,
     userId: user?.id
   });
   ```

3. **Network Trace**
   - Open browser DevTools
   - Go to Network tab
   - Reproduce the issue
   - Export as HAR file

<Callout type="info">
  For real-time assistance, join our Discord community where you can get help from both the development team and experienced users.
</Callout>
