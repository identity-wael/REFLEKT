---
title: Migration Guides
description: Step-by-step guides for migrating between versions and platforms
---

import { Steps, Callout } from 'nextra/components'

# Migration Guides

Comprehensive guides for upgrading Earna AI Console and migrating between platforms.

## Version Information

<Callout type="info">
**Current Version**: Earna AI Console is running on Vercel AI SDK v5 with full TypeScript strict mode support. See [AI SDK v5 Implementation](/console/development/ai-sdk-v5) for details.
</Callout>

## Version Upgrades

### From v1.x to v2.x

<Callout type="warning">
**Breaking Changes**: This upgrade includes breaking changes to the database schema and API structure.
</Callout>

<Steps>
### Backup Your Data

```bash
# Export conversations
pnpm export:conversations

# Backup Supabase database
pg_dump $DATABASE_URL > backup-v1.sql
```

### Update Dependencies

```bash
# Update to latest versions
pnpm add next@15 @ai-sdk/openai@latest ai@latest
pnpm add @supabase/supabase-js@latest
```

### Database Migration

```sql
-- Add new columns
ALTER TABLE conversations ADD COLUMN model TEXT DEFAULT 'gpt-4o';
ALTER TABLE users ADD COLUMN daily_message_count INT DEFAULT 0;
ALTER TABLE users ADD COLUMN is_pro BOOLEAN DEFAULT false;

-- Update existing data
UPDATE conversations SET model = 'gpt-4o' WHERE model IS NULL;
```

### Environment Variables

```env
# New required variables
ENCRYPTION_KEY=your-32-byte-hex-key
NEXTAUTH_SECRET=your-secret

# Updated variable names
OPENAI_API_KEY=sk-proj-...  # Changed format
```

### Code Updates

```typescript
// Old way (v1.x)
import { OpenAI } from 'openai';
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// New way (v2.x)
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
```

</Steps>

## Platform Migrations

### From Custom Backend to Supabase

<Steps>
### Export Existing Data

```javascript
// Export users
const users = await db.collection('users').find({}).toArray();
const usersCSV = convertToCSV(users);

// Export conversations
const conversations = await db.collection('conversations').find({}).toArray();
const conversationsCSV = convertToCSV(conversations);
```

### Set Up Supabase

```bash
# Install Supabase CLI
pnpm add -g @supabase/cli

# Initialize project
supabase init

# Start local development
supabase start
```

### Create Schema

```sql
-- Create tables
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email TEXT UNIQUE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  title TEXT,
  model TEXT DEFAULT 'gpt-4o',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### Import Data

```bash
# Upload CSVs to Supabase
supabase db import users.csv
supabase db import conversations.csv
```

</Steps>

### From Firebase to Supabase

<Steps>
### Export Firebase Data

```javascript
// firestore-export.js
const admin = require('firebase-admin');
const fs = require('fs');

admin.initializeApp({
  credential: admin.credential.cert(serviceAccount),
});

const db = admin.firestore();

async function exportCollection(collectionName) {
  const snapshot = await db.collection(collectionName).get();
  const data = snapshot.docs.map(doc => ({
    id: doc.id,
    ...doc.data()
  }));

  fs.writeFileSync(`${collectionName}.json`, JSON.stringify(data, null, 2));
}

exportCollection('users');
exportCollection('conversations');
```

### Transform Data Format

```javascript
// transform-data.js
const firebase_users = require('./users.json');
const supabase_users = firebase_users.map(user => ({
  id: user.uid,
  email: user.email,
  created_at: user.createdAt._seconds * 1000
}));

fs.writeFileSync('supabase_users.json', JSON.stringify(supabase_users));
```

### Import to Supabase

```sql
-- Use Supabase Dashboard or SQL
INSERT INTO users (id, email, created_at)
SELECT
  (data->>'id')::uuid,
  data->>'email',
  to_timestamp((data->>'created_at')::bigint / 1000)
FROM json_array_elements($1::json) AS data;
```

</Steps>

## AI Provider Migrations

### From OpenAI Legacy to OpenAI v4

<Steps>
### Update API Key Format

```env
# Old format
OPENAI_API_KEY=sk-...

# New format (if applicable)
OPENAI_API_KEY=sk-proj-...
```

### Update Code

```typescript
// Old way
import OpenAI from 'openai';
const openai = new OpenAI();

const completion = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: messages,
  stream: true,
});

// New way
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const result = await streamText({
  model: openai('gpt-4o'),
  messages: messages,
});
```

### Test Migration

```bash
# Run migration test
pnpm test:migration

# Check API connectivity
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models
```

</Steps>

### Adding New AI Providers

<Steps>
### Install Provider SDK

```bash
pnpm add @ai-sdk/anthropic  # For Claude
pnpm add @ai-sdk/google     # For Gemini
```

### Configure Environment

```env
ANTHROPIC_API_KEY=sk-ant-api03-...
GOOGLE_AI_API_KEY=AIzaSy...
```

### Update Model Configuration

```typescript
// lib/ai/models.ts
import { anthropic } from '@ai-sdk/anthropic';

export const modelConfigs = {
  // ... existing models
  'claude-3-opus': {
    provider: 'anthropic',
    model: anthropic('claude-3-opus-20240229'),
    name: 'Claude 3 Opus',
    contextWindow: 200000,
  }
};
```

</Steps>

## Deployment Migrations

### From Local to Vercel

<Steps>
### Prepare Environment Variables

```bash
# Export current environment
env | grep -E '^(OPENAI|SUPABASE|ANTHROPIC)' > .env.production
```

### Install Vercel CLI

```bash
pnpm add -g vercel
vercel login
```

### Deploy

```bash
# Link project
vercel link

# Set environment variables
vercel env add OPENAI_API_KEY production
vercel env add SUPABASE_URL production

# Deploy
vercel --prod
```

### Configure Custom Domain

```bash
vercel domains add console.yourdomain.com
```

</Steps>

### From Vercel to Self-Hosted

<Steps>
### Prepare Docker Setup

```dockerfile
# Dockerfile
FROM node:22-alpine

WORKDIR /app
COPY package*.json ./
RUN pnpm install --prod --frozen-lockfile

COPY . .
RUN pnpm build

EXPOSE 3000
CMD ["pnpm", "start"]
```

### Set Up Environment

```bash
# docker-compose.yml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SUPABASE_URL=${SUPABASE_URL}
```

### Deploy

```bash
docker-compose up -d
```

</Steps>

## Troubleshooting Migrations

### Common Issues

**Database Connection Errors**
```bash
# Test connection
psql $NEW_DATABASE_URL -c "SELECT 1"

# Check firewall
telnet your-db-host 5432
```

**API Key Issues**
```bash
# Verify key format
echo $OPENAI_API_KEY | cut -c1-10  # Should show sk-proj-

# Test API access
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models
```

**Environment Variable Problems**
```bash
# Check all env vars
printenv | grep -E '^(NEXT_|SUPABASE|OPENAI)'

# Validate required vars
node -e "require('./lib/env').validateEnv()"
```

### Rollback Procedures

**Database Rollback**
```sql
-- Restore from backup
psql $DATABASE_URL < backup-v1.sql

-- Or use Supabase backup
supabase db reset
```

**Code Rollback**
```bash
# Git rollback
git revert HEAD

# Or reset to previous version
git reset --hard v1.0.0
```

**Deployment Rollback**
```bash
# Vercel rollback
vercel rollback

# Docker rollback
docker-compose down
docker-compose up -d previous-image
```

<Callout type="info">
**Need Help?** If you encounter issues during migration, check our [Troubleshooting Guide](/operations/troubleshooting) or reach out on [Discord](https://discord.gg/earna-ai).
</Callout>
